---
title: '1. Теория'
description: 'Основные принципы измерения количества информации и введение в понятие энтропии.'
---

## 1.1. Теоретические сведения

Количество информации должно обладать свойствами меры (положительность, монотонность, аддитивность).

Если событие $x$ может наступить с вероятностью $p$, то количество информации о наступлении этого события равно $I(x) = -\log p$.
Основание логарифма определяет только единицу измерения. Мы будем в дальнейшем использовать двоичные логарифмы, а количество информации измерять в битах.

Получение информации уменьшает неопределенность при принятии решений. Мерой неопределенности является **энтропия**.

Пусть источник информации (сообщений) $X$ может находиться в одном из состояний (посылать одно из сообщений) $x_1, x_2, \dots, x_N$ с вероятностями $p_1, p_2, \dots, p_N$. При этом $\sum_{i=1}^N p_i = 1$. Это задается таблицей:

$$
X = \begin{pmatrix} x_1 & x_2 & \dots & x_N \\ p_1 & p_2 & \dots & p_N \end{pmatrix}
$$

Энтропия этого источника равна $H(X) = -\sum_{i=1}^N p_i \log p_i$, т.е. средневзвешенному количеству информации от всех возможных состояний. Чем больше число вариантов $N$, тем больше неопределенность.

Количество информации, содержащееся в сообщении, равно разности энтропий до и после получения сообщения (см. [пример 3](/path/to/example-3)). <!-- Замените на правильный путь -->

Для вычисления количества информации и энтропии удобно использовать программу обработки электронных таблиц OpenOffice Calc.

В первый столбец вводятся заданные по условию задачи вероятности, а во второй – формулы вида `=-LOG(A2;2)` для количества информации или `=-A2*LOG(A2;2)` для энтропии. В последнем случае необходимо добавить суммирование по столбцам (см. [пример 2](/path/to/example-2)). <!-- Замените на правильный путь --> Заметим, что программа Calc не всегда правильно выбирает количество отображаемых разрядов. Вероятности принято считать с точностью 4 знака после запятой, а энтропию – округлять до трёх знаков.