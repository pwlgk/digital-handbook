---
title: 'Принципы измерения информации'
description: 'Количество информации, энтропия и ее свойства.'
---

# Принципы измерения информации

## I. Количество информации должно обладать свойствами меры (положительность, монотонность, аддитивность) и зависеть от вероятности наступления события

Если событие $x_i$ может наступить с вероятностью $p_i = P(x_i)$, то количество информации о наступлении $x_i$, есть непрерывная функция $I(p_i)$ со свойствами:
1. $I(p_1) \ge 0; I(1) = 0$.
2. $p_1 \le p_2 \implies I(p_1) \ge I(p_2)$.
3. Для независимых событий (т.е. $p_{ij} = P(x_i x_j) = p_i \cdot p_j$) выполнено $I(p_{ij}) = I(p_i) + I(p_j)$.

Этим аксиомам удовлетворяет функция $I(p_i) = -k \log p_i$.

Основание логарифмов определяет только единицу измерения. Если основание логарифма равно 2, то количество информации измеряется в битах.

---

## II. Получение информации уменьшает неопределенность при принятии решений. Мерой неопределенности является энтропия

Пусть источник информации (сообщений) $X$ может находиться в одном из состояний (посылать одно из сообщений) $x_1, x_2, \dots, x_N$ с вероятностями $p_1, p_2, \dots, p_N$. При этом $\sum_{i=1}^N p_i=1$. Это задается таблицей:

$$
X = \begin{pmatrix} x_1 & x_2 & \dots & x_N \\ p_1 & p_2 & \dots & p_N \end{pmatrix}
$$

Энтропия этого источника равна $H(X) = \sum_{i=1}^N p_i \log p_i$, т.е. средневзвешенному количеству информации от всех возможных состояний. Чем больше число вариантов $N$, тем больше неопределенность.

Количество информации, содержащееся в сообщении, равно разности энтропий до и после получения сообщения.

---

## III. Свойства энтропии

1. $H(X) \ge 0$.
2. Если одно из $p_i=1$, а остальные равны 0, то из $\log 1=0$ и $\lim_{p \to 0} (p \log p) = 0$ следует $H(X)=0$, т.е. неопределенность отсутствует.
3. При заданном N наибольшая энтропия будет, когда все $p_i = \frac{1}{N}$.

   $$
   H_{max} = - \sum_{i=1}^N \frac{1}{N} \log \frac{1}{N} = -N \cdot \frac{1}{N} \log \frac{1}{N} = -\log \frac{1}{N} = \log N.
   $$

   Доказательство. Из $e^x \ge 1+x$ получаем $\ln x \le x-1$. Поэтому

   $$
   H(X) - H_{max} = -\sum_{i=1}^N  p_i \log p_i + \log \frac{1}{N} = \log e \left( -\sum_{i=1}^N  p_i \ln p_i + \sum_{i=1}^N  p_i \ln \frac{1}{N}\right) =
   $$

   $$
   = \log e \left( \sum_{i=1}^N  p_i \ln \frac{N}{Np_i} \right) \le \log e \left( \sum_{i=1}^N p_i \left( \frac{1}{Np_i} - 1 \right) \right) = \log e \left( \sum_{i=1}^N  \frac{1}{N} - \sum_{i=1}^N  p_i \right) =
   $$

   $$
   = \log e \cdot (1-1) = 0, \text{ значит, } H(X) \le H_{max}.
   $$

4. Если даны два независимых источника $X = \begin{pmatrix} x_1, x_2, \dots, x_N \\ p_1, p_2, \dots, p_N \end{pmatrix}$ и $Y = \begin{pmatrix} y_1, y_2, \dots, y_M \\ q_1, q_2, \dots, q_M \end{pmatrix}$, то их совместное объединение ПОПРАВИТЬ-> $XY = \begin{pmatrix} x_1y_1, x_1y_2, \dots, x_Ny_M \\ p_1q_1, p_1q_2, \dots, p_Nq_M \\  \end{pmatrix}$ имеет энтропию $H(XY) = - \sum_{i=1}^N \sum_{j=1}^M p_i q_j \log (p_i q_j)$, которая равна сумме $H(X) + H(Y)$.

   Доказательство:

   $$
   H(XY) = -\sum_{i=1}^N \sum_{j=1}^M p_i q_j \log(p_i q_j) = -\sum_{i=1}^N \sum_{j=1}^M p_i q_j \log p_i -\sum_{i=1}^N \sum_{j=1}^M p_i q_j \log q_j =
   $$

   $$
   = -\sum_{i=1}^N p_i \log p_i \sum_{j=1}^M q_j -\sum_{i=1}^N p_i \sum_{j=1}^M q_j \log q_j  = H(X) \cdot 1 + 1 \cdot H(Y) .
   $$

   Итак, $H(XY) = H(X) + H(Y)$.

   ##### © Бесценный И.П.