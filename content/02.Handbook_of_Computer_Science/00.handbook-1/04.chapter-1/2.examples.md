---
title: '2. Примеры'
description: 'Примеры расчета количества информации и энтропии для различных источников.'
---

## 1.2. Примеры

### Пример 1.

На стадионе эмулировано проведение футбольного матча. Рассматриваются следующие возможные исходы:
*   матч состоится в назначенное время ($p1=0.89$);
*   матч начнется с незначительным опозданием ($p2=0.084$);
*   матч не состоится из-за неявки одной из команд ($p3=0.012$);
*   матч будет перенесен по решению Федерации ($p4=0.008$);
*   матч не состоится из-за погодных условий ($p5=0.005$);
*   матч не состоится из-за политических событий ($p6=0.001$).
Вычислить количество собственной информации, доставляемое каждым из исходов, и их вклад в общую энтропию источника информации.

Запускаем программу OpenOffice Calc. В первый столбец вводим заданные вероятности:

|   | A      |
|---|--------|
| **1** | P      |
| **2** | 0.8900 |
| **3** | 0.0840 |
| **4** | 0.0120 |
| **5** | 0.0080 |
| **6** | 0.0050 |
| **7** | 0.0010 |

В ячейку `B2` вводим формулу `=-LOG(A2;2)` и, используя маркер автозаполнения, протягиваем до конца второго столбца.

|   | A      | B      | C |
|---|--------|--------|---|
| **1** | P      | H      |   |
| **2** | 0.8900 | 0.1691 |   |
| **3** | 0.0840 | 3.5739 |   |
| **4** | 0.0120 | 6.3808 |   |
| **5** | 0.0080 | 6.9658 |   |
| **6** | 0.0050 | 7.6439 |   |
| **7** | 0.0010 | 9.9658 |   |
| **8** |        |        |   |

Количество собственной информации, доставляемое каждым из исходов, получено. Вклад в общую энтропию рассчитаем в третьем столбце. В ячейку `C2` вводим формулу `=-A2*LOG(A2;2)` и, используя маркер автозаполнения, протягиваем до конца третьего столбца.

|   | A      | B      | C      | D |
|---|--------|--------|--------|---|
| **1** | P      | H      | H      |   |
| **2** | 0.8900 | 0.1691 | 0.1496 |   |
| **3** | 0.0840 | 3.5739 | 0.3002 |   |
| **4** | 0.0120 | 6.3808 | 0.0766 |   |
| **5** | 0.0080 | 6.9658 | 0.0557 |   |
| **6** | 0.0050 | 7.6439 | 0.0382 |   |
| **7** | 0.0010 | 9.9658 | 0.0100 |   |
| **8** |        |        | 0.6303 |   |

Общая энтропия H есть сумма чисел третьего столбца.

Полученные результаты позволяют сделать такие выводы: чем меньше вероятность наступления события, тем больше количество информации, однако вклад в общую энтропию у редких событий незначителен. В этом примере наибольший вклад (почти половину) дает второе событие ($p2=0.084$).

### Пример 2.

Сравнить энтропии источников информации, заданных распределениями вероятностей состояний:
а) $X_1, X_2, X_3$ - $0.4, 0.3, 0.1, 0.2$
б) $X_1, X_2, X_3, X_4$ - $0.25, 0.25, 0.25, 0.25$

Так же, как в [примере 1](/path/to/example-1), в первый столбец вводим заданные вероятности. Поскольку нужно найти только энтропию, в ячейку B2 вводим сразу формулу `=-A2*LOG(A2;2)` и, используя маркер автозаполнения, протягиваем до конца второго столбца.

|   | A      | B      | C |
|---|--------|--------|---|
| **1** | P      | H      |   |
| **2** | 0.4000 | 0.5288 |   |
| **3** | 0.3000 | 0.5211 |   |
| **4** | 0.1000 | 0.3322 |   |
| **5** | 0.2000 | 0.4644 |   |
| **6** | 1.000  | 1.8465 |   |

Общая энтропия H есть сумма чисел второго столбца. Заодно просуммируем и первый столбец, чтобы убедиться в том, что сумма заданных вероятностей равна 1.
Для источника а) энтропия равна 1.846. Теперь просто впечатаем другие вероятности, и перерасчет по формулам происходит автоматически.

|   | A      | B      | C |
|---|--------|--------|---|
| **1** | P      | H      |   |
| **2** | 0.2500 | 0.5000 |   |
| **3** | 0.2500 | 0.5000 |   |
| **4** | 0.2500 | 0.5000 |   |
| **5** | 0.2500 | 0.5000 |   |
| **6** | 1.000  | 2.000  |   |

Для источника б) энтропия равна 2.0. Наибольшая неопределенность достигается для равновероятных состояний.

### Пример 3.

Сообщения источника x1, x2, x3, x4 для согласования с каналом кодируются в соответствии с таблицей:

|       | x1  | x2  | x3  | x4  |
| :---- | :-: | :-: | :-: | :-: |
| p(xi) | 1/2 | 1/4 | 1/8 | 1/8 |
| Код   | 000 | 011 | 101 | 100 |

Пусть на вход поступает код «011». Вычислить количество информации, которую содержит каждый последовательный символ кода.

Рассчитываем начальную энтропию:

|   | A      | B      |
|---|--------|--------|
| **1** | P      | H      |
| **2** | 0.5000 | 0.5000 |
| **3** | 0.2500 | 0.5000 |
| **4** | 0.1250 | 0.3750 |
| **5** | 0.1250 | 0.3750 |
| **6** | 1.000  | 1.750  |

До поступления кода энтропия равна $H_0=1.75$. После получения первого символа кода (это "0") вероятности получения сообщений изменяются. Так как X3 и X4 начинаются с "1", то вероятность их получения становится равной 0. Вероятности получения сообщений X1 и X2 в сумме должны давать 1, но их отношение сохраняется (первая в два раза больше второй). Единственный способ добиться того, чтобы величины давали в сумме 1 с сохранением соотношений, – поделить каждую на их общую сумму:
$1/2 \to \frac{1/2}{1/2+1/4} = \frac{2}{3}$; $1/4 \to \frac{1/4}{1/2+1/4} = \frac{1}{3}$.

Рассчитываем энтропию после получения первого символа кода $H_1=0.918$.

|   | A      | B      |
|---|--------|--------|
| **1** | P      | H      |
| **2** | 0.6667 | 0.3900 |
| **3** | 0.3333 | 0.5283 |
| **4** | 0.0000 | 0.0000 |
| **5** | 0.0000 | 0.0000 |
| **6** | 1.000  | 0.9183 |

Количество информации, содержащейся в первом символе кода $I_1 = H_0-H_1 = 1.75 - 0.918 = 0.832$.

После получения второго символа кода (это "1") вероятности получения сообщений X1, X3 и X4 равняются 0. Вероятность получения сообщения X2 должна быть равна 1. Энтропия теперь находится без компьютера. $H_2=0*1=0*1=0$.

Количество информации, содержащейся во втором символе кода $I_2 = H_1-H_2 = 0.918 - 0 = 0.918$.

После получения третьего символа кода (это "1") вероятность получения сообщения X2 остается равной 1. Энтропия равна $H_3=0$.

Количество информации, содержащейся в третьем символе кода $I_3 = H_2-H_3 = 0-0=0$. Это означает, что последний символ кода в данном случае не несет информации, он просто дублирует второй символ.

   ##### © Бесценный И.П.