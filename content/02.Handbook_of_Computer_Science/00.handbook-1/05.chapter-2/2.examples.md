---
title: '2. Примеры'
description: 'Примеры расчетов совместной, условной энтропии и взаимной информации.'
---

## 2.2. Примеры

### Пример 1.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0,4 | 0,1 | 0 |
| **x₂** | 0 | 0,2 | 0,1 |
| **x₃** | 0 | 0 | 0,2 |

Запускаем программу OpenOffice Calc и вводим заданные вероятности:

|   | A      | B   | C   | D   |
|:--|:-------|:----|:----|:----|
| **1** | P(X,Y) | Y1  | Y2  | Y3  |
| **2** | X1     | 0,4 | 0,1 | 0   |
| **3** | X2     | 0   | 0,2 | 0,1 |
| **4** | X3     | 0   | 0   | 0,2 |

Находим безусловные вероятности путем суммирования по строкам и по столбцам.

|   | A      | B   | C   | D   | E      |
|:--|:-------|:----|:----|:----|:-------|
| **1** | P(X,Y) | Y1  | Y2  | Y3  | P(Xi)  |
| **2** | X1     | 0,4 | 0,1 | 0   | **0,5**  |
| **3** | X2     | 0   | 0,2 | 0,1 | **0,3**  |
| **4** | X3     | 0   | 0   | 0,2 | **0,2**  |
| **5** | P(Yj)  | **0,4** | **0,3** | **0,3** |        |

Условные вероятности находятся путем деления. Обратите внимание на адресацию ячеек, относительную по одной координате и абсолютную по другой. Это нужно для автозаполнения. Точность устанавливаем 4 знака после запятой.

|   | F       | G       | H       |
|:--|:--------|:--------|:--------|
| **1** | P(Yj|Xi) | Y1      | Y2      | Y3      |
| **2** | X1      | 0,8000  | 0,2000  | 0,0000  |
| **3** | X2      | 0,0000  | 0,6667  | 0,3333  |
| **4** | X3      | 0,0000  | 0,0000  | 1,0000  |
| **5** | P(Xi|Yj) |         |         |         |
| **6** | X1      | 1,0000  | 0,3333  | 0,0000  |
| **7** | X2      | 0,0000  | 0,6667  | 0,3333  |
| **8** | X3      | 0,0000  | 0,0000  | 0,6667  |

Теперь находим логарифмы (со знаком минус) всех найденных вероятностей. Чтобы избежать сообщений об ошибке (логарифм 0 не определен), используется условие IF.

|   | I      | J      | K      | L      | M      | N      | O      |
|:--|:-------|:-------|:-------|:-------|:-------|:-------|:-------|
| **1** | LOG    |        |        | LOG    |        |        |        |
| **2** | 0,322  | 2,322  | 0,000  | 1,000  | 0,322  | 2,322  | 0,000  |
| **3** | 0,000  | 0,585  | 1,585  | 1,737  | 0,585  | 1,585  | 1,585  |
| **4** | 0,000  | 0,000  | 0,000  | 2,322  | 2,322  | 0,000  | 0,000  |
| **5** |        |        |        |        |        |        |        |
| **6** | 1,322  | 1,737  | 1,737  |        |        |        |        |
| **7** | 1,585  | 0,000  | 0,000  |        |        |        |        |
| **8** | 0,000  | 0,585  | 0,585  |        |        |        |        |

По формулам находим требуемые величины. Функция `SUMPRODUCT()` сразу вычисляет сумму попарных произведений из двух диапазонов электронной таблицы одинакового размера.

Результаты расчетов получаются такие:
|    |                        |
|:---|:-----------------------|
| **9**  |                        |
| **10** | H(X,Y) = 2,122         |
| **11** | H(X) = 1,485           |
| **12** | H(Y) = 1,571           |
| **13** | H(Y|X) = 0,636         |
| **14** | H(X|Y) = 0,551         |
| **15** | I(X,Y) = 0,934         |

Проверки по формулам соотношения между совместной и другими энтропиями:
$H(X,Y) = 1,485 + 0,636 = 1,571 + 0,551 = 2,122$.
Взаимная информация $I(X,Y) = 1,485 - 0,551 = 1,571 - 0,636 = 0,934$.
Расхождение в 0,001 в пределах ошибки округления.

### Пример 2.

По каналу связи передаются сообщения $x_1, x_2, x_3, x_4$ с вероятностями $P(x_1)=0,4; P(x_2)=0,3; P(x_3)=0,2; P(x_4)=0,1$. Из-за помех принимаемые сигналы появляются с условными вероятностями, заданными в таблице:

| P(yⱼ|xᵢ) | y₁ | y₂ | y₃ | y₄ |
| :--- | :-: | :-: | :-: | :-: |
| **x₁** | 0,8 | 0,1 | 0,1 | 0 |
| **x₂** | 0,2 | 0,5 | 0,3 | 0 |
| **x₃** | 0 | 0,2 | 0,7 | 0,1 |
| **x₄** | 0 | 0,2 | 0,2 | 0,6 |

Найти энтропию на входе и на выходе канала, количество переданной, посторонней и утерянной информации.
Так же, как в [примере 1](/path/to/example-1), вводим заданные вероятности. Только безусловные вероятности Y находим по формуле полной вероятности. Обратите внимание на адресацию в функции `SUMPRODUCT()`.

|   | A      | B   | C   | D   | E   |
|:--|:-------|:----|:----|:----|:----|
| **1** | P(x)   | 0,4 | 0,8 | 0,1 | 0,1 |
| **2** |        | 0,3 | 0,2 | 0,5 | 0,3 |
| **3** |        | 0,2 | 0   | 0,2 | 0,7 |
| **4** |        | 0,1 | 0   | 0,2 | 0,2 |
| **5** | P(y)   | 0,40| 0,23| 0,25| 0,12|

Теперь находим логарифмы (со знаком минус) всех найденных вероятностей (для использования условных IF см. [пример 1](/path/to/example-1)). Условная энтропия H(Y|X) получается в два этапа (см. [формулу](/path/to/formula)).

|   | F      | G      | H      | I      |
|:--|:-------|:-------|:-------|:-------|
| **1** | H(Y|x) | LOG    |        |        |
| **2** | 0,922  | 0,322  | 3,322  | 3,322  |
| **3** | 1,485  | 2,322  | 1,000  | 1,737  |
| **4** | 1,157  | 0,000  | 2,322  | 0,515  |
| **5** | 1,371  | 0,000  | 2,322  | 0,000  |
| **6** | 1,183  | 1,322  | 2,128  | 3,059  |

Результат для H(Y|X) можно получить дальнейшим протягиванием ячеек с формулами для безусловных вероятностей Y.
Результаты расчетов получаются такие:
|    |                      |
|:---|:---------------------|
| **7**  |                      |
| **8**  | H(X) = 1,846         |
| **9**  | H(Y) = 1,884         |
| **10** | H(Y|X) = 1,183       |
| **11** | H(X,Y) = 3,029       |
| **12** | H(X|Y) = 1,146       |
| **13** | I(X,Y) = 0,701       |

Запишем ответ:
*   энтропия на входе канала H(X)=1,846,
*   энтропия на выходе канала H(Y)=1,884,
*   количество утерянной информации H(X|Y)=1,146,
*   количество посторонней информации H(Y|X)=1,183,
*   количество переданной информации I(X,Y)=0,701.

   ##### © Бесценный И.П.