---
title: '3. Задания'
description: 'Задания для самостоятельной работы по теме "Условная энтропия и взаимная информация".'
---

## 2.3. Задания для самостоятельной работы

### Вариант 1.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0,2 | 0,1 | 0 |
| **x₂** | 0 | 0,3 | 0,1 |
| **x₃** | 0 | 0 | 0,3 |

### Вариант 2.

По каналу связи передаются сообщения $x_1, x_2, x_3, x_4$ с вероятностями $P(x_1)=0.4, P(x_2)=0.3, P(x_3)=0.2, P(x_4)=0.1$. Из-за помех принимаемые сигналы появляются с условными вероятностями, заданными в таблице:

| P(yⱼ|xᵢ) | y₁ | y₂ | y₃ | y₄ |
| :--- | :-: | :-: | :-: | :-: |
| **x₁** | 0,98 | 0,01 | 0 | 0,01 |
| **x₂** | 0,02 | 0,95 | 0,02 | 0 |
| **x₃** | 0 | 0,02 | 0,97 | 0,01 |
| **x₄** | 0,02 | 0 | 0,02 | 0,96 |

Найти энтропию на входе и на выходе канала, количество переданной, посторонней и утерянной информации.

### Вариант 3.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0,5 | 0,05 | 0 |
| **x₂** | 0 | 0,2 | 0,05 |
| **x₃** | 0,1 | 0 | 0,1 |

### Вариант 4.

По каналу связи передаются сообщения $x_1, x_2, x_3, x_4$ с вероятностями $P(x_1)=0.4, P(x_2)=0.3, P(x_3)=0.2, P(x_4)=0.1$. Из-за помех принимаемые сигналы появляются с условными вероятностями, заданными в таблице:

| P(yⱼ|xᵢ) | y₁ | y₂ | y₃ | y₄ |
| :--- | :-: | :-: | :-: | :-: |
| **x₁** | 0,99 | 0,01 | 0 | 0 |
| **x₂** | 0,13 | 0,81 | 0,02 | 0,04 |
| **x₃** | 0,15 | 0,03 | 0,8 | 0,02 |
| **x₄** | 0,05 | 0 | 0,02 | 0,93 |

Найти энтропию на входе и на выходе канала, количество переданной, посторонней и утерянной информации.

### Вариант 5.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0,1 | 0,2 | 0,2 |
| **x₂** | 0,3 | 0 | 0,2 |
| **x₃** | 0 | 0 | 0 |

### Вариант 6.

По каналу связи передаются сообщения $x_1, x_2, x_3, x_4$ с вероятностями $P(x_1)=0.4, P(x_2)=0.3, P(x_3)=0.2, P(x_4)=0.1$. Из-за помех принимаемые сигналы появляются с условными вероятностями, заданными в таблице:

| P(yⱼ|xᵢ) | y₁ | y₂ | y₃ | y₄ |
| :--- | :-: | :-: | :-: | :-: |
| **x₁** | 0,82 | 0,05 | 0,13 | 0 |
| **x₂** | 0,05 | 0,92 | 0,03 | 0 |
| **x₃** | 0 | 0,03 | 0,94 | 0,03 |
| **x₄** | 0,13 | 0 | 0,01 | 0,86 |

Найти энтропию на входе и на выходе канала, количество переданной, посторонней и утерянной информации.

### Вариант 7.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0,1 | 0,4 | 0 |
| **x₂** | 0 | 0,1 | 0,2 |
| **x₃** | 0,2 | 0 | 0 |

### Вариант 8.

По каналу связи передаются сообщения $x_1, x_2, x_3, x_4$ с вероятностями $P(x_1)=0.4, P(x_2)=0.3, P(x_3)=0.2, P(x_4)=0.1$. Из-за помех принимаемые сигналы появляются с условными вероятностями, заданными в таблице:

| P(yⱼ|xᵢ) | y₁ | y₂ | y₃ | y₄ |
| :--- | :-: | :-: | :-: | :-: |
| **x₁** | 0,97 | 0,01 | 0 | 0,02 |
| **x₂** | 0,01 | 0,96 | 0,03 | 0 |
| **x₃** | 0 | 0,01 | 0,98 | 0,01 |
| **x₄** | 0,01 | 0 | 0 | 0,99 |

Найти энтропию на входе и на выходе канала, количество переданной, посторонней и утерянной информации.

### Вариант 9.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0 | 0,1 | 0,4 |
| **x₂** | 0 | 0,1 | 0,2 |
| **x₃** | 0,1 | 0 | 0,1 |

### Вариант 10.

По каналу связи передаются сообщения $x_1, x_2, x_3, x_4$ с вероятностями $P(x_1)=0.4, P(x_2)=0.3, P(x_3)=0.2, P(x_4)=0.1$. Из-за помех принимаемые сигналы появляются с условными вероятностями, заданными в таблице:

| P(yⱼ|xᵢ) | y₁ | y₂ | y₃ | y₄ |
| :--- | :-: | :-: | :-: | :-: |
| **x₁** | 0,98 | 0 | 0,02 | 0 |
| **x₂** | 0 | 0,95 | 0 | 0,05 |
| **x₃** | 0,03 | 0 | 0,97 | 0 |
| **x₄** | 0 | 0,04 | 0 | 0,96 |

Найти энтропию на входе и на выходе канала, количество переданной, посторонней и утерянной информации.

### Вариант 11.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0 | 0,1 | 0,2 |
| **x₂** | 0,3 | 0 | 0,1 |
| **x₃** | 0,1 | 0,2 | 0 |

### Вариант 12.

По каналу связи передаются сообщения $x_1, x_2, x_3, x_4$ с вероятностями $P(x_1)=0.4, P(x_2)=0.3, P(x_3)=0.2, P(x_4)=0.1$. Из-за помех принимаемые сигналы появляются с условными вероятностями, заданными в таблице:

| P(yⱼ|xᵢ) | y₁ | y₂ | y₃ | y₄ |
| :--- | :-: | :-: | :-: | :-: |
| **x₁** | 0,88 | 0,12 | 0 | 0 |
| **x₂** | 0 | 0,85 | 0 | 0,15 |
| **x₃** | 0,13 | 0 | 0,87 | 0 |
| **x₄** | 0 | 0,14 | 0 | 0,86 |

Найти энтропию на входе и на выходе канала, количество переданной, посторонней и утерянной информации.

### Вариант 13.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0 | 0,2 | 0,1 |
| **x₂** | 0,1 | 0 | 0,3 |
| **x₃** | 0,2 | 0,1 | 0 |

### Вариант 14.

По каналу связи передаются сообщения $x_1, x_2, x_3, x_4$ с вероятностями $P(x_1)=0.4, P(x_2)=0.3, P(x_3)=0.2, P(x_4)=0.1$. Из-за помех принимаемые сигналы появляются с условными вероятностями, заданными в таблице:

| P(yⱼ|xᵢ) | y₁ | y₂ | y₃ | y₄ |
| :--- | :-: | :-: | :-: | :-: |
| **x₁** | 0,82 | 0,1 | 0,08 | 0 |
| **x₂** | 0 | 0,84 | 0,07 | 0,09 |
| **x₃** | 0,12 | 0 | 0,86 | 0,02 |
| **x₄** | 0,08 | 0,04 | 0 | 0,88 |

Найти энтропию на входе и на выходе канала, количество переданной, посторонней и утерянной информации.

### Вариант 15.

Задано совместное распределение вероятностей появления сообщений двух источников. Найти совместную, условные и безусловные энтропии, а также взаимную информацию.

| P(xᵢ,yⱼ) | y₁ | y₂ | y₃ |
| :--- | :-: | :-: | :-: |
| **x₁** | 0,1 | 0,05 | 0 |
| **x₂** | 0 | 0,1 | 0,05 |
| **x₃** | 0 | 0 | 0,7 |

   ##### © Бесценный И.П.