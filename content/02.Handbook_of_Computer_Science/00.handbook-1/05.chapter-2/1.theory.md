---
title: '1. Теория'
description: 'Совместные и условные вероятности, совместная и условная энтропия, взаимная информация.'
---

## 2.1. Теоретические сведения

Пусть два источника сообщений $X$ и $Y$ связаны между собой. Обозначим $P(x_i)$ и $P(y_j)$ вероятности соответствующих сообщений; $P(x_i, y_j)$ – вероятность одновременного появления сообщений $x_i$ и $y_j$ (совместная вероятность); $P(x_i|y_j)$ – вероятность появления сообщения $x_i$ при условии, что было получено сообщение $y_j$ (условная вероятность).
Если задана матрица совместных вероятностей, то все ее элементы в сумме дают 1:

$$
\sum_{i=1}^N \sum_{j=1}^M P(x_i, y_j) = 1.
$$

При этом $\sum_{i=1}^N P(x_i, y_j) = P(y_j)$ и $\sum_{j=1}^M P(x_i, y_j) = P(x_i)$. Чтобы получить условные вероятности, используют формулы $P(x_i|y_j) = \frac{P(x_i, y_j)}{P(y_j)}$ и $P(y_j|x_i) = \frac{P(x_i, y_j)}{P(x_i)}$. Потом можно найти совместную энтропию:

$$
H(XY) = -\sum_{i=1}^N \sum_{j=1}^M P(x_i, y_j) \cdot \log P(x_i, y_j).
$$

и условные энтропии:

$$
H(Y|X) = -\sum_{i=1}^N \sum_{j=1}^M P(x_i, y_j) \cdot \log P(y_j|x_i).
$$

$$
H(X|Y) = -\sum_{i=1}^N \sum_{j=1}^M P(x_i, y_j) \cdot \log P(x_i|y_j).
$$

[Пример 1](/path/to/example-1) показывает, как проводить вычисления в этом случае.

Если же задана матрица условных вероятностей (канальная матрица), то сумма элементов каждой строки равна $\sum_{j=1}^M P(y_j|x_i)=1$.
Необходимо еще задать безусловные вероятности $P(x_i)$, тогда

$$
H(Y|X) = -\sum_{i=1}^N P(x_i) \cdot \left(\sum_{j=1}^M P(y_j|x_i) \cdot \log P(y_j|x_i) \right).
$$

Выражение в скобках есть условная энтропия $Y$ при каждом конкретном значении $x_i$: $H(Y|x_i) = -\sum_{j=1}^M P(y_j|x_i) \cdot \log P(y_j|x_i)$.
Итак, $H(Y|X) = \sum_{i=1}^N P(x_i) \cdot H(Y|x_i)$.
Для вычисления энтропии $H(Y)$ используют формулу полной вероятности $P(y_j) = \sum_{i=1}^N P(x_i) \cdot P(y_j|x_i)$.

[Пример 2](/path/to/example-2) относится ко второму способу задания вероятностей.

В обоих случаях задания связи источников $X$ и $Y$ из соотношений:
$H(XY) = H(X) + H(Y|X) = H(Y) + H(X|Y)$,
$I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$,
$H(XY) = H(X) + H(Y) - I(X,Y)$,
зная три величины, можно найти другие три величины.

Для процесса передачи информации найденные величины означают следующее:

::MermaidDiagram
---
code: |
  graph LR
    subgraph source [Источник]
        direction LR
        A["Энтропия на входе<br>канала H(X)"]
    end
    
    subgraph channel [Канал]
        direction LR
        B["Переданная информация<br>I(X,Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)"]
    end
    
    subgraph receiver [Приемник]
        direction LR
        C["Энтропия на выходе<br>канала H(Y)"]
    end

    A -- "H(X|Y)<br>Утечка информации" --> B
    B -- "H(Y|X)<br>Посторонняя информация" --> C
---
::

Таким образом, взаимная информация $I(X,Y)$ определяет количество информации, переданное по каналу.

   ##### © Бесценный И.П.