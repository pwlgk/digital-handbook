---
title: '2. Примеры'
---

# Примеры к Главе 1

## Пример 1: Энтропия монеты

Рассмотрим честную монету ("орел" или "решка"). Вероятность каждого исхода `p = 0.5`.

Энтропия такой системы равна:
`H = - (0.5 * log₂(0.5) + 0.5 * log₂(0.5)) = 1 бит`

Это означает, что результат одного броска несет в себе ровно 1 бит информации.

## Пример 2: Алфавит с неравными вероятностями

Дан алфавит `A = {a, b, c}` с вероятностями `P(a)=0.5`, `P(b)=0.25`, `P(c)=0.25`.
Подсветка синтаксиса для кода:
```javascript
function calculateEntropy(probabilities) {
  return probabilities.reduce((sum, p) => {
    if (p > 0) {
      return sum - p * Math.log2(p);
    }
    return sum;
  }, 0);
}

const probs = [0.5, 0.25, 0.25];
console.log(`Энтропия: ${calculateEntropy(probs)} бит/символ`); // Выведет 1.5